{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dashscope import MultiModalConversation\n",
    "from http import HTTPStatus\n",
    "from PIL import Image\n",
    "import dashscope\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "dashscope.api_key = 'sk-fc58a205ddfb4a718bf904619f3aba59'\n",
    "# os.system('export DASHSCOPE_API_KEY=)\n",
    "def extract_box(text, w, h):\n",
    "    pattern = r'\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    box = []\n",
    "    for match in matches:\n",
    "        box += match.split(',')\n",
    "    for i in range(len(box)):\n",
    "        box[i] = eval(box[i])\n",
    "    box[0] = int(box[0] / 1000 * w)\n",
    "    box[1] = int(box[1] / 1000 * h)\n",
    "    box[2] = int(box[2] / 1000 * w)\n",
    "    box[3] = int(box[3] / 1000 * h)\n",
    "    return box\n",
    "def reasoning_grouding_by_qwen(file_path, text_prompt):\n",
    "    \"\"\"Sample of use local file.\n",
    "       linux&mac file schema: file:///home/images/test.png\n",
    "       windows file schema: file://D:/images/abc.png\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\n",
    "        'role': 'system',\n",
    "        'content': [{\n",
    "            'text': '''\n",
    "                    You are an AI assistant who is good at making accurate vision grounding based on questions asked\n",
    "                    '''\n",
    "        }]\n",
    "    }, {\n",
    "        'role':\n",
    "        'user',\n",
    "        'content': [\n",
    "            {\n",
    "                'image': f'file://{file_path}'\n",
    "            },\n",
    "            {\n",
    "                'text': text_prompt\n",
    "            },\n",
    "        ]\n",
    "    }]\n",
    "    response = MultiModalConversation.call(model='qwen-vl-chat-v1', messages=messages)\n",
    "    image = Image.open(file_path)\n",
    "    \n",
    "    # print(response.output.choices)\n",
    "    # answer = response.output.choices[0].message.content[0]['box']\n",
    "    answer = response.output.choices[0].message.content\n",
    "    # result_image = response.output.choices[0].message.content[1]['result_image']\n",
    "    # result_image = Image.open(requests.get(result_image, stream=True).raw)\n",
    "    box = extract_box(answer, *(image.size))\n",
    "    return answer, box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"ckpts/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'data/nerf_llff_data/room/images_4/DJI_20200226_143850_006.JPG'\n",
    "# file_path = 'data/nerf_llff_data/fern/images_4/IMG_4026.JPG'\n",
    "# prompt = ''\n",
    "# text_prompt = \n",
    "# answer, box = reasoning_grouding_by_qwen(file_path, text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene = 'fern'\n",
    "# prompt = 'the biggest tree'\n",
    "# scene = 'flower'\n",
    "# prompt = 'what looks the most colorful'\n",
    "# scene = 'fortress'\n",
    "# prompt = 'what seems to be made of metal'\n",
    "scene = 'horns'\n",
    "prompt = 'which is the biggest skeleton'\n",
    "# prompt = 'what can be used to play videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_root = f'data/nerf_llff_data/{scene}/images_4'\n",
    "image_names = os.listdir(images_root)\n",
    "image_names = [image_name for idx, image_name in enumerate(image_names) if idx % 8 == 0]\n",
    "image_paths = [os.path.join(images_root, image_name) for image_name in image_names]\n",
    "\n",
    "text_prompt = f'Please grounding <ref> {prompt} </ref>'\n",
    "save_path = f'output/qwen_sam/{scene}/{prompt}'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "# image_names = os.listdir(images_root)\n",
    "# image_paths = [os.path.join(images_root, name) for name in image_names]\n",
    "# images = [Image.open(path) for path in image_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMG_1800.JPG',\n",
       " 'IMG_1808.JPG',\n",
       " 'IMG_1816.JPG',\n",
       " 'IMG_1824.JPG',\n",
       " 'IMG_1832.JPG',\n",
       " 'IMG_1840.JPG']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:14<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(image_paths))):\n",
    "    image_name = image_names[i]\n",
    "    image_path = image_paths[i]\n",
    "    image = Image.open(image_path)\n",
    "    answer, box = reasoning_grouding_by_qwen(image_path, text_prompt)\n",
    "    box = np.array(box)\n",
    "    # result_list, mask_result_list, mask_list, mask_rgb_list, output_str = lisa_pipeline(lisa_text_prompt, image=image)\n",
    "    predictor.set_image(np.array(image))\n",
    "    masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=box[None, :],\n",
    "    multimask_output=False,\n",
    ")\n",
    "    save_name = image_name.split('.')[0] + '.png'\n",
    "    Image.fromarray(masks[0]).save(os.path.join(save_path, save_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
